{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSzaookw1glp"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def BIO_labels(raw_words, aspects):\n",
        "    words = raw_words.split()\n",
        "    labels = ['O'] * len(words)\n",
        "    for aspect in aspects:\n",
        "        labels[aspect['from']:aspect['to']] = ['B'] + ['I'] * (aspect['to'] - aspect['from'] - 1)\n",
        "    return labels\n",
        "\n",
        "with open('Laptop_Review_Test.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "processed_data = {}\n",
        "\n",
        "for i, entry in enumerate(data):\n",
        "    processed_data[str(i+1)] = {'text': entry['raw_words'], 'labels': BIO_labels(entry['raw_words'], entry['aspects'])}\n",
        "\n",
        "with open('Laptop_Review_Test_BIO.json', 'w') as file:\n",
        "    json.dump(processed_data, file, indent=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTiEZfw1jFqz",
        "outputId": "89f6dea1-6da3-4b02-c194-b3ebe5887575"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['In', 'the', 'shop', ',', 'these', 'MacBooks', 'are', 'encased', 'in', 'a', 'soft', 'rubber', 'enclosure', '-', 'so', 'you', 'will', 'never', 'know', 'about', 'the', 'razor', 'edge', 'until', 'you', 'buy', 'it', ',', 'get', 'it', 'home', ',', 'break', 'the', 'seal', 'and', 'use', 'it', '(', 'very', 'clever', 'con', ')', '.']\n"
          ]
        }
      ],
      "source": [
        "wr = \"In the shop , these MacBooks are encased in a soft rubber enclosure - so you will never know about the razor edge until you buy it , get it home , break the seal and use it ( very clever con ) .\"\n",
        "print(wr.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Shxdbflaqg_s"
      },
      "outputs": [],
      "source": [
        "##PART1A STRATIFIED SPLIT\n",
        "\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "with open('NER_TRAIN_JUDGEMENT.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "filtered_data = [item for item in data if item.get('annotations') and item['annotations'][0].get('result')]\n",
        "\n",
        "labels = []\n",
        "for item in filtered_data:\n",
        "\n",
        "    label = item['annotations'][0]['result'][0]['value']['labels'][0]\n",
        "    labels.append(label)\n",
        "\n",
        "train_data, test_data = train_test_split(filtered_data, test_size=0.15, stratify=labels, random_state=42)\n",
        "\n",
        "with open('NER_TRAIN_JUDGEMENT_SPLIT.json', 'w') as f_train:\n",
        "    json.dump(train_data, f_train, indent=4)\n",
        "\n",
        "with open('NER_VAL_JUDGEMENT_SPLIT.json', 'w') as f_val:\n",
        "    json.dump(test_data, f_val, indent=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model = Sequential()\n",
        "# model.add(embedding_layer)\n",
        "\n",
        "# model.add(LSTM(EMBEDDING_DIM, dropout=0.2, recurrent_dropout=0.2))\n",
        "\n",
        "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'"
      ],
      "metadata": {
        "id": "D4eiAdBDO3tZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#final code #part 1A #for the given example there must be 16 tokens idk how #rest everything is perfect\n",
        "#FIXED BUT GOTTA CHECK ONCE AGAIN\n",
        "\n",
        "import json\n",
        "\n",
        "# Load the dataset\n",
        "with open('NER_TEST_JUDGEMENT.json', 'r') as f:\n",
        "    dataset = json.load(f)\n",
        "\n",
        "# Define the legal entities\n",
        "legal_entities = [\"JUDGE\", \"PERSON\", \"ORGANIZATION\", \"LOCATION\", \"DATE\", \"TIME\", \"MONEY\", \"PERCENT\", \"LAW\", \"CRIME\", \"ORDINAL\", \"QUANTITY\", \"CARDINAL\"]\n",
        "\n",
        "# Function to assign BIO labels to tokens\n",
        "def bio_labels(text, annotations):\n",
        "    tokens = text.split(\" \")\n",
        "    labels = ['O'] * len(tokens)  # Initialize all labels as 'O'\n",
        "\n",
        "    for annotation in annotations:\n",
        "        for result in annotation['result']:\n",
        "            start = result['value']['start']\n",
        "            end = result['value']['end']\n",
        "            label = result['value']['labels'][0]\n",
        "            entity_text = text[start:end]\n",
        "\n",
        "            entity_tokens = entity_text.split()\n",
        "            token_index = 0\n",
        "            for i, token in enumerate(tokens):\n",
        "                if token_index >= len(entity_tokens):\n",
        "                    break\n",
        "                if entity_tokens[token_index] in token:\n",
        "                    labels[i] = 'B_' + label if token_index == 0 else 'I_' + label\n",
        "                    token_index += 1\n",
        "\n",
        "    return labels\n",
        "\n",
        "# Process each data entry and assign BIO labels\n",
        "processed_data = {entry['id']: {'text': entry['data']['text'], 'labels': bio_labels(entry['data']['text'], entry['annotations'])} for entry in dataset}\n",
        "\n",
        "# Save processed data to a new JSON file\n",
        "with open('NER_TEST.json', 'w') as f:\n",
        "    json.dump(processed_data, f, indent=4)\n"
      ],
      "metadata": {
        "id": "ummnvmedvf6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#code with ID fixedd #part 1A\n",
        "\n",
        "import json\n",
        "\n",
        "with open('NER_TRAIN_JUDGEMENT_SPLIT.json', 'r') as f:\n",
        "    dataset = json.load(f)\n",
        "\n",
        "legal_entities = [\"JUDGE\", \"PERSON\", \"ORGANIZATION\", \"LOCATION\", \"DATE\", \"TIME\", \"MONEY\", \"PERCENT\", \"LAW\", \"CRIME\", \"ORDINAL\", \"QUANTITY\", \"CARDINAL\"]\n",
        "\n",
        "def bio_labels(text, annotations):\n",
        "    tokens = text.split()\n",
        "    labels = ['O'] * len(tokens)\n",
        "\n",
        "    for annotation in annotations:\n",
        "        for result in annotation['result']:\n",
        "            start, end = result['value']['start'], result['value']['end']\n",
        "            label = result['value']['labels'][0]\n",
        "            entity_tokens = text[start:end].split()\n",
        "            for i, token in enumerate(tokens):\n",
        "                if any(entity_token in token for entity_token in entity_tokens):\n",
        "                    labels[i] = 'B_' + label if i == tokens.index(entity_tokens[0]) else 'I_' + label\n",
        "\n",
        "    return labels\n",
        "\n",
        "processed_data = [{'id': entry['id'], 'text': entry['data']['text'], 'labels': bio_labels(entry['data']['text'], entry['annotations'])} for entry in dataset]\n",
        "\n",
        "with open('NER_TRAIN.json', 'w') as f:\n",
        "    json.dump(processed_data, f, indent=4)\n"
      ],
      "metadata": {
        "id": "AhAuoxNbDeUW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}